// Полная база из 65 уникальных вопросов (объединение Word, JS и фото)
const questions = [
    // --- ВАРИАНТ 1 (1-32) ---
    { text: "How many significant digits are in $8.7 \\times 10^3$?", options: ["2", "4", "5", "1"], correct: 0 },
    { text: "How many significant digits are in $4.800$?", options: ["4", "2", "3", "5"], correct: 0 },
    { text: "How many significant digits are in $4.6 \\times 10^2$?", options: ["2", "5", "4", "3"], correct: 0 },
    { text: "How many significant digits are in $1.7 \\times 10^3$?", options: ["2", "4", "5", "1"], correct: 0 },
    { text: "In floating point, machine epsilon measures:", options: ["Smallest relative spacing", "Underflow limit", "Overflow limit", "Smallest representable number"], correct: 0 },
    { text: "Relative error for true $x=47.75$ and approximate $\\tilde{x}=42.96$ is:", options: ["10.03%", "11.15%", "9.82%", "10.50%"], correct: 0 },
    { text: "Which is a measure of how much the output can change for a small change in input?", options: ["Condition number", "Relative error", "Absolute error", "Machine epsilon"], correct: 0 },
    { text: "Iterative refinement improves:", options: ["Accuracy", "Stability only", "Memory", "Runtime"], correct: 0 },
    { text: "Cramer's rule is computationally efficient for very large systems ($n > 100$).", options: ["False", "True"], correct: 0 },
    { text: "Orthogonal matrices have condition number:", options: ["One", "Infinite", "Zero", "Undefined"], correct: 0 },
    { text: "In Gaussian elimination, 'Partial Pivoting' involves swapping rows to move the largest value to pivot position.", options: ["True", "False"], correct: 0 },
    { text: "Condition number $\\kappa(A)$ depends on:", options: ["Matrix $A$ only", "$x$ only", "$b$ only", "$A$ and $b$"], correct: 0 },
    { text: "The Gauss-Jordan method transforms the matrix into an Upper Triangular form.", options: ["False", "True"], correct: 0 },
    { text: "The residual vector in $Ax = b$ is defined as:", options: ["$Ax - b$", "$A - b$", "$bx - A$", "$A \\cdot x$"], correct: 0 },
    { text: "A high condition number implies:", options: ["Ill-conditioned system", "No solution", "Orthogonal matrix", "Well-conditioned system"], correct: 0 },
    { text: "Matrix norm measures:", options: ["Size of matrix", "Determinant", "Condition number", "Eigenvalues"], correct: 0 },
    { text: "A rectangular system ($m \\times n$) may have:", options: ["Any of the above (No, One, or Many solutions)", "No solution", "One solution", "Many solutions"], correct: 0 },
    { text: "Orthogonal matrices satisfy:", options: ["$A^T A = I$", "$A = I$", "$A^2 = I$", "$\\det(A) = 0$"], correct: 0 },
    { text: "What is the main difference between Gaussian elimination and Gauss-Jordan elimination?", options: ["Gauss gives upper triangular, Jordan gives identity/reduced row echelon", "Gauss gives diagonal matrix", "No difference", "Gauss is only for 2x2"], correct: 0 },
    { text: "The error matrix $E$ in the iterative method for matrix inversion is often defined as:", options: ["$E = AB - I$", "$E = I - AB$", "$E = A + B$", "$E = A^{-1} - B$"], correct: 0 },
    { text: "The Lagrange basis polynomial $L_i(x)$ for point $(x_i, y_i)$ has what property?", options: ["$L_i(x_i) = 1$ and $L_i(x_j) = 0$ for $j \\neq i$", "$L_i(x_i) = y_i$", "$L_i(x)$ integrates to 1", "$L_i(x_i) = 0$"], correct: 0 },
    { text: "Which interpolation method has local control - changing one point affects only nearby intervals?", options: ["Cubic spline interpolation", "Lagrange interpolation", "Newton's forward", "Divided differences"], correct: 0 },
    { text: "For cubic splines, we typically enforce continuity of:", options: ["Function, first, and second derivatives", "Only the function values", "All derivatives up to fourth order", "First derivative only"], correct: 0 },
    { text: "The first divided difference $f[x_0, x_1]$ is defined as:", options: ["$(f(x_1) - f(x_0)) / (x_1 - x_0)$", "$f(x_1) - f(x_0)$", "$(f(x_1) + f(x_0)) / 2$", "$(f(x_0) - f(x_1)) / dx"], correct: 0 },
    { text: "The formula for Newton's forward interpolation for $u = (x - x_0)/h$ is:", options: ["$P(x) = y_0 + u\\Delta y_0 + u(u-1)/2! \\Delta^2 y_0 + ...$", "$P(x) = y_n + u\\nabla y_n + ...$", "$P(x) = uy_0 + u(u+1)/2! \\Delta y_0$", "None"], correct: 0 },
    { text: "Degree of interpolating polynomial for $n+1$ data points is at most:", options: ["$n$", "$n+1$", "$n-1$", "$2n$"], correct: 0 },
    { text: "Newton forward/backward formulas typically assume data is:", options: ["Equally spaced", "Randomly spaced", "Increasing $y$ only", "Complex numbers"], correct: 0 },
    { text: "Natural boundary conditions for a cubic spline at endpoints are:", options: ["$S''(x_0) = 0, S''(x_n) = 0$", "$S'(x_0) = 0$", "$S(x_0) = 0$", "None"], correct: 0 },
    { text: "In least squares, $R^2$ close to 1 means:", options: ["Good fit", "Poor fit", "No correlation", "Negative correlation"], correct: 0 },
    { text: "If correlation coefficient $r = 0$, this implies:", options: ["No linear correlation", "Perfect positive correlation", "No relationship of any kind", "$R^2 = 1$"], correct: 0 },
    { text: "Machine epsilon for 64-bit double precision is approximately:", options: ["$2.2 \\times 10^{-16}$", "$1.1 \\times 10^{-7}$", "$10^{-32}$", "$0$"], correct: 0 },
    { text: "Condition number depends on:", options: ["Matrix $A$ only", "Vector $b$ only", "$A$ and $b$", "Number of iterations"], correct: 0 },

    // --- ВАРИАНТ 2 (33-65) ---
    { text: "Which method can find complex roots without using complex initial guesses?", options: ["Müller's method", "Bisection", "Newton-Raphson", "Secant", "False position"], correct: 0 },
    { text: "In the Secant method, the formula for $x_{n+1}$ is:", options: ["$x_n - f(x_n) \\cdot \\frac{x_n - x_{n-1}}{f(x_n) - f(x_{n-1})}$", "$x_n - f(x_n)/f'(x_n)$", "$(x_n + x_{n-1})/2$", "$g(x_n)$"], correct: 0 },
    { text: "The Newton-Raphson method convergence is usually:", options: ["Quadratic", "Linear", "Superlinear", "Cubic"], correct: 0 },
    { text: "A polynomial of degree $n$ has at most:", options: ["$n$ roots", "$n-1$ roots", "No roots", "Infinite roots"], correct: 0 },
    { text: "Which condition guarantees convergence for both Jacobi and Gauss-Seidel?", options: ["$A$ is strictly diagonally dominant", "$A$ is symmetric positive definite", "$A$ is tridiagonal", "$\\rho(T) < 1$"], correct: 0 },
    { text: "For system $4x+y=7, x+3y=5$, $x$ after one Gauss-Seidel step from $(0,0)$ is:", options: ["1.750", "1.000", "1.500", "1.167"], correct: 0 },
    { text: "The relaxation method with $\\omega = 1$ is equivalent to:", options: ["Gauss-Seidel method", "Jacobi method", "SOR", "Power method"], correct: 0 },
    { text: "Why is Gauss-Seidel typically faster than Jacobi?", options: ["Uses updated values immediately", "Less memory", "Always converges", "Easier to parallelize"], correct: 0 },
    { text: "The primary goal of the Power method is:", options: ["Find the dominant eigenvalue and eigenvector", "Compute matrix inverse", "Factorize into LU", "Find all roots"], correct: 0 },
    { text: "The Jacobi method (for eigenvalue problem) is applicable to:", options: ["Symmetric matrices", "Any square matrix", "Identity matrices", "Singular matrices"], correct: 0 },
    { text: "In the Jacobi eigenvalue method, rotation angle $\\theta$ satisfies:", options: ["$\\tan(2\\theta) = 2a_{pq} / (a_{pp} - a_{qq})$", "$\\sin(2\\theta) = a_{pq}$", "$\\cos(2\\theta) = 1$", "$\\theta = 45^\\circ$"], correct: 0 },
    { text: "Central difference formula accuracy is of order:", options: ["Second order $O(h^2)$", "First order $O(h)$", "Third order", "Fourth order"], correct: 0 },
    { text: "Picard's method is based on:", options: ["Iterative solution of integral equation", "Numerical differentiation", "Polynomial interpolation", "Difference equation"], correct: 0 },
    { text: "For 9 equally spaced points, Boole's rule is applied:", options: ["Twice (first 5 and last 5 points)", "Three times", "Once over all", "Never"], correct: 0 },
    { text: "The correct formula for $k_3$ in RK4 is:", options: ["$h \\cdot f(x_n + h/2, y_n + k_2/2)$", "$h \\cdot f(x_n + h, y_n + k_2)$", "$h \\cdot f(x_n + h/2, y_n + k_1/2)$", "$h \\cdot f(x_n + h/2, y_n + k_3)$"], correct: 0 },
    { text: "Simpson's 3/8 rule uses the formula:", options: ["$3h/8 [f_0 + 3f_1 + 3f_2 + f_3]$", "$h/3 [f_0 + 4f_1 + f_2]$", "$2h/45 [7f_0 + 32f_1 + 12f_2 + 32f_3 + 7f_4]$", "$h/2 [f_0 + f_1]$"], correct: 0 },
    { text: "Newton-Cotes formulas are based on:", options: ["Replacing integrand with an interpolating polynomial", "Random sampling", "Derivatives at ends", "Least squares"], correct: 0 },
    { text: "For $y'=y, y(0)=1, h=0.5$, find $y(1.0)$ using Euler's method.", options: ["2.25", "2.0", "2.5", "1.5"], correct: 0 },
    { text: "Weddle's rule is generally more accurate than Simpson because:", options: ["It uses a higher degree interpolating polynomial", "It uses fewer points", "It is for complex functions", "Step size is smaller"], correct: 0 },
    { text: "The backward difference operator $\\nabla^2 y_n$ is:", options: ["$y_n - 2y_{n-1} + y_{n-2}$", "$y_n - y_{n-1}$", "$y_{n+1} - 2y_n + y_{n-1}$", "$y_n + 2y_{n-1} + y_{n-2}$"], correct: 0 },
    { text: "What will function $func(A)$ return for $A = [[3, 1, 1], [0, 4, 1], [1, 1, 5]]$?", options: ["True", "False", "Error", "None"], correct: 0 },
    { text: "Bisection method interval update code logic:", options: ["if $f(a)*f(c) < 0: b=c$ else: $a=c$", "$a, b = b, c$", "$a = c, b = c$", "$mid = (a+b)/2$"], correct: 0 },
    { text: "Gaussian elimination code usually returns matrix $A$ as:", options: ["Upper triangular", "Lower triangular", "Identity", "Diagonal"], correct: 0 },
    { text: "In LU decomposition, the most likely error in Line 2 is:", options: ["Division by zero pivot", "Incorrect sum limits", "Missing identity", "No error"], correct: 0 },
    { text: "Back substitution loop range in Python is:", options: ["range(n-1, -1, -1)", "range(n)", "range(1, n)", "range(n, 0, -1)"], correct: 0 },
    { text: "Trapezoidal rule code for $f(x)=x^2, [0,2], n=4$ returns:", options: ["2.75", "2.66", "2.25", "3.0"], correct: 0 },
    { text: "What is being calculated here?", code: "for i in range(1, n):\n    diff = table[i-1]\n    cur = [diff[j+1] - diff[j] for j in range(n-i)]\n    table.append(cur)", options: ["Forward difference table", "Backward difference table", "Divided difference table", "Lagrange table"], correct: 0 },
    { text: "3rd order RK formula uses which pattern for $k_2$?", options: ["$k_2 = hf(x_n+h/2, y_n+k_1/2)$", "$k_2 = hf(x_n+h, y_n+k_1)$", "$k_2 = hf(x_n+h/3, y_n+k_1/3)$", "$k_2 = hf(x_n+2h/3, y_n+2k_1/3)$"], correct: 0 },
    { text: "For $y' = x^2 + y, y(0)=1$, find $y''(0)$ using Taylor series.", options: ["1", "2", "0", "3"], correct: 0 },
    { text: "The error in Simpson's 1/3 rule depends on which derivative?", options: ["Fourth derivative $f^{(4)}$", "Second derivative", "First derivative", "Third derivative"], correct: 0 },
    { text: "Which method is a multi-step method?", options: ["Adams-Bashforth", "RK4", "Euler", "Picard"], correct: 0 },
    { text: "Richardson extrapolation is used to:", options: ["Improve accuracy of differentiation/integration", "Find roots", "Solve СЛАУ", "Fit curves"], correct: 0 },
    { text: "In the bisection method, error after $n$ iterations is proportional to:", options: ["$1/2^n$", "$1/n$", "$1/n^2$", "$\\log(n)$"], correct: 0 }
];